{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NnOoFtyHilj"
      },
      "source": [
        "# Human Activity Recognition Using WiFi Signals\n",
        "\n",
        "## Overview\n",
        "Human Activity Recognition (HAR) using WiFi signals leverages the unique properties of wireless channel variations to detect different activities.\n",
        "\n",
        "## Data Format\n",
        "- **WiFi signal data** is similar to image data in structure, represented in the shape `(channels, height, width)`, but with a different interpretation:\n",
        "  - `channels` → **channel**\n",
        "  - `height` → **Time Steps**\n",
        "  - `width` → **Antenna Pairs (transmitter-receiver combinations)**\n",
        "- **Labels** represent a predefined set of classes, as is typical in classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GqPqJNJIWNg"
      },
      "source": [
        "# Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnmdeTv2xPQ2",
        "outputId": "0ca678cd-f8c5-4869-e604-f99eefefec8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/alihabibullah/question-2-data?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 204M/204M [00:04<00:00, 44.2MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/alihabibullah/question-2-data/versions/1\n"
          ]
        }
      ],
      "source": [
        "#1\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"alihabibullah/question-2-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8hne08gxqjN",
        "outputId": "466820a1-d3c0-4ada-ba72-1f183d9ff053"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['WiFiSensingDataset.pt']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.listdir(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEZvoIOGxqWc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DwIkG1iHVPg",
        "outputId": "629fbf5c-7130-4a8c-fbd3-a4f73dd57569"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-0656954f04d1>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(f\"{path}/WiFiSensingDataset.pt\")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Load the .pt file\n",
        "data = torch.load(f\"{path}/WiFiSensingDataset.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_z7GTT7pUFJc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5zRa_cooQRs_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert labels to LongTensor\n",
        "data['y_train'] = data['y_train'].type(torch.LongTensor)\n",
        "data['y_test'] = data['y_test'].type(torch.LongTensor)\n",
        "\n",
        "# Concatenate train and test data\n",
        "inputs = torch.cat((data['X_train'], data['X_test']))\n",
        "targets = torch.cat((data['y_train'], data['y_test']))\n",
        "\n",
        "# Create TensorDataset\n",
        "mydataset = TensorDataset(inputs, targets)\n",
        "\n",
        "# Split into train and test datasets\n",
        "train_size = int(0.8 * len(mydataset))\n",
        "test_size = len(mydataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(mydataset, [train_size, test_size])\n",
        "\n",
        "# DataLoader for batching\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vkoyStvJBLl"
      },
      "source": [
        "# Task 1: Analyze the Dataset ( Stored in `data`)\n",
        "\n",
        "1. **Determine the number of unique labels** in the dataset.  \n",
        "\n",
        "2. **Determine the shape of the input data** (number of samples and features).  \n",
        "\n",
        "3. **Find the maximum value** in the dataset.  \n",
        "\n",
        "4. **Find the minimum value** in the dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lPNBr_UK1nb",
        "outputId": "a4ca1fbb-5233-4d36-deda-17f01198ad38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape inputs: torch.Size([1, 250, 90])\n",
            "Num unique labels: 7\n",
            "Maximum inputs: 1.0\n",
            "Maximum targets: 6\n",
            "Minimum inputs: 0.0\n",
            "Minimum targets: 0\n"
          ]
        }
      ],
      "source": [
        "# -Determine the shape of the input data (number of samples and features)\n",
        "shape_input = inputs[0].shape\n",
        "print(f\"Shape inputs: {shape_input}\")\n",
        "\n",
        "# -Determine the number of unique labels\n",
        "unique_labels = torch.unique(targets)\n",
        "num_unique_labels = len(unique_labels)\n",
        "print(f\"Num unique labels: {num_unique_labels}\")\n",
        "\n",
        "# -Find the maximum value in the dataset\n",
        "max_value = torch.max(inputs).item()\n",
        "max_target_value = torch.max(targets).item()\n",
        "print(f\"Maximum inputs: {max_value}\")\n",
        "print(f\"Maximum targets: {max_target_value}\")\n",
        "\n",
        "# -Find the minimum value in the dataset\n",
        "min_value = torch.min(inputs).item()\n",
        "min_target_value = torch.min(targets).item()\n",
        "print(f\"Minimum inputs: {min_value}\")\n",
        "print(f\"Minimum targets: {min_target_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbbW9TqJJI84"
      },
      "source": [
        "# Task 2: Build and Evaluate a Neural Network\n",
        "\n",
        "1. **Design a Neural Network (Maximum 5 Layers)**  \n",
        "   Build a compact neural network with no more than 5 layers. Clearly specify the type of each layer (e.g., Dense, Conv2D) and any activation functions used.\n",
        "\n",
        "2. **Evaluate Your Model**  \n",
        "   Train your network on the provided dataset and report the evaluation metrics (e.g., accuracy, loss). Discuss the performance of your model and any challenges faced during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xUNlzzV3Pq-t"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class NN4Layer(nn.Module):\n",
        "  def __init__(self, num_inp):\n",
        "\n",
        "    super(NN4Layer, self).__init__()\n",
        "\n",
        "    self.layer_1 = nn.Linear(num_inp, 512)\n",
        "    self.layer_2 = nn.Linear(512, 256)\n",
        "    self.layer_3 = nn.Linear(256, 128)\n",
        "    self.layer_4 = nn.Linear(128, 10)\n",
        "\n",
        "    self.hidden_activation = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    z1 = self.layer_1(x)\n",
        "    a1 = self.hidden_activation(z1)\n",
        "\n",
        "    z2 = self.layer_2(a1)\n",
        "    a2 = self.hidden_activation(z2)\n",
        "\n",
        "    z3 = self.layer_3(a2)\n",
        "    a3 = self.hidden_activation(z3)\n",
        "\n",
        "    z4 = self.layer_4(a3)\n",
        "\n",
        "    return z4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzIypBtyRlDP",
        "outputId": "1d15f578-0408-4fc0-e8f7-221feaf88045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cpu\n"
          ]
        }
      ],
      "source": [
        "# Training configuration\n",
        "num_epochs = 20\n",
        "lr = 0.001\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Initialize model\n",
        "model = NN4Layer(1*250*90)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()  # multi-class\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(f'Using device {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAMgZ-FXJIL9",
        "outputId": "2a039d90-f4ff-4689-c693-c824937a4925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 and the loss 1.9666107892990112\n",
            "Epoch 2 and the loss 1.783217191696167\n",
            "Epoch 3 and the loss 1.6803579330444336\n",
            "Epoch 4 and the loss 1.5769826173782349\n",
            "Epoch 5 and the loss 1.5370591878890991\n",
            "Epoch 6 and the loss 1.4663300514221191\n",
            "Epoch 7 and the loss 1.395700454711914\n",
            "Epoch 8 and the loss 1.324356198310852\n",
            "Epoch 9 and the loss 1.3826854228973389\n",
            "Epoch 10 and the loss 1.2359778881072998\n",
            "Epoch 11 and the loss 1.2868398427963257\n",
            "Epoch 12 and the loss 1.191057562828064\n",
            "Epoch 13 and the loss 1.1548160314559937\n",
            "Epoch 14 and the loss 1.1275105476379395\n",
            "Epoch 15 and the loss 1.0915288925170898\n",
            "Epoch 16 and the loss 1.0967615842819214\n",
            "Epoch 17 and the loss 1.0997689962387085\n",
            "Epoch 18 and the loss 1.044012427330017\n",
            "Epoch 19 and the loss 0.9838751554489136\n",
            "Epoch 20 and the loss 0.9661014676094055\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0\n",
        "  for features,labels in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    # ReShape\n",
        "    features = features.reshape(-1,250*90).to(device)\n",
        "    labels = labels.to(device)\n",
        "    # forward: prediction\n",
        "    prediction = model(features)\n",
        "    loss = criterion(prediction, labels)\n",
        "    total_loss += loss\n",
        "  # backward\n",
        "    loss.backward()\n",
        "  # update weights and take a step\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "  print(f'Epoch {epoch+ 1} and the loss {total_loss/len(train_loader)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsixL-b9wdDL"
      },
      "source": [
        "to test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0CDeVFAT6z8",
        "outputId": "7853a7fa-e0b8-4df3-db12-5b75b3912e6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 67.33%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "# Testing loop\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        features = features.reshape(-1, 250*90).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        prediction = model(features)\n",
        "\n",
        "        # Get the predicted labels (class with max probability)\n",
        "        _, predicted_labels = torch.max(prediction, 1)\n",
        "\n",
        "        # Collect true and predicted labels\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy on test set: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5PHWz6VtLLT1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pqoK28DLPB4"
      },
      "source": [
        "thanks to Ahmed Y. Radwan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5er_pdi8w5cV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
